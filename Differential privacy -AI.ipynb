{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55391a7",
   "metadata": {},
   "source": [
    "## AI PRIVACY USING DIFFERENTIAL PRIVACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f038ab",
   "metadata": {},
   "source": [
    "### PROBLEM STATEMENT:\n",
    "Data anonymization is the process of removing personal identifiers, both direct and indirect, that may lead to an individual being identified while training AI/ML models. This will help organizations maintain confidentiality and AI privacy.\n",
    "To protect sensitivity of data which holds sensitive information, intelligent solutions should be optimized with necessary privacy frameworks and accelerators using *Differential privacy*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5adb0",
   "metadata": {},
   "source": [
    "Differential privacy allows to avail a facilty in obtaining the useful information without divulging the private information or identiification about an individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607166f",
   "metadata": {},
   "source": [
    "Differential privacy enables to solve this problem by adding \"noise\" to the data that user can't identify any individual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a14d5",
   "metadata": {},
   "source": [
    "#### IMPORT NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717186f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow  as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea853f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca90420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31fbbe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c942d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84914395",
   "metadata": {},
   "source": [
    "### Load and pre-procee the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aed8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "train_data, train_labels = train\n",
    "test_data, test_labels = test\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "assert train_data.min() == 0.\n",
    "assert train_data.max() == 1.\n",
    "assert test_data.min() == 0.\n",
    "assert test_data.max() == 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea330b",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b5d3a",
   "metadata": {},
   "source": [
    "epochs - It means one complete pass of the training dataset through the algorithm.\n",
    "\n",
    "\n",
    "\n",
    "Batch size - It is the number of training examples utilized in one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2cb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae517f4",
   "metadata": {},
   "source": [
    "1.12_norm_clip - The maximum Euclidean (L2) norm of each gradient that is applied to update model parameters. This hyperparameter is used to bound the optimizer's sensitivity to individual training points.\n",
    "\n",
    "2.Noise_multiplier - It is used to add noise to the gradients during training to increase the privacy.\n",
    "\n",
    "3.microbatches - Each batch of data is split in smaller units called microbatches. By default, each microbatch should contain a single training example. This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch.\n",
    "\n",
    "4.Learning rate - Tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b4797df",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 1.3\n",
    "num_microbatches = 250\n",
    "learning_rate = 0.25\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3049588",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c52fa187",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 8,\n",
    "                           strides=2,\n",
    "                           padding='same',\n",
    "                           activation='relu',\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(2, 1),\n",
    "    tf.keras.layers.Conv2D(32, 4,\n",
    "                           strides=2,\n",
    "                           padding='valid',\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2, 1),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f58a6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "#import optimizers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2e1b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow_privacy.version import __version__\n",
    "if hasattr(sys, 'skip_tf_privacy_import'):  \n",
    "    # Useful for standalone scripts.\n",
    "  pass\n",
    "else:\n",
    "  # TensorFlow v1 imports\n",
    "  from tensorflow_privacy import v1\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4d329",
   "metadata": {},
   "source": [
    "Define the optimizer and loss function for the learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71f0530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =DPGradientDescentGaussianOptimizer (\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "799fc4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675abe0d",
   "metadata": {},
   "source": [
    "### Train  the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b8a477b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 1.1065 - acc: 0.6580"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1194s 20ms/sample - loss: 1.1065 - acc: 0.6580 - val_loss: 0.6778 - val_acc: 0.8064\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 1152s 19ms/sample - loss: 0.5444 - acc: 0.8523 - val_loss: 0.4322 - val_acc: 0.8890\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 1203s 20ms/sample - loss: 0.4190 - acc: 0.8977 - val_loss: 0.3506 - val_acc: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x217359eb8b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          validation_data=(test_data, test_labels),\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad555b",
   "metadata": {},
   "source": [
    "Two metrics are used to express the DP guarantee of an ML algorithm:\n",
    "\n",
    "Delta () - Bounds the probability of the privacy guarantee not holding. A rule of thumb is to set it to be less than the inverse of the size of the training dataset. In this tutorial, it is set to 10^-5 as the MNIST dataset has 60,000 training points.\n",
    "Epsilon () - This is the privacy budget. It measures the strength of the privacy guarantee by bounding how much the probability of a particular model output can vary by including (or excluding) a single training point. A smaller value for  implies a better privacy guarantee. However, the  value is only an upper bound and a large value could still mean good privacy in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d2dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.417% and noise_multiplier = 1.3 iterated over 720 steps satisfies differential privacy with eps = 0.79 and delta = 1e-05.\n",
      "The optimal RDP order is 18.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7903529309843027, 18.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=train_data.shape[0],\n",
    "                                              batch_size=batch_size,\n",
    "                                              noise_multiplier=noise_multiplier,\n",
    "                                              epochs=epochs,\n",
    "                                              delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "873a8b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.417% and noise_multiplier = 1.3 iterated over 3600 steps satisfies differential privacy with eps = 1.18 and delta = 1e-05.\n",
      "The optimal RDP order is 17.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.179900673982703, 17.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=250, noise_multiplier=1.3, epochs=15, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe29822",
   "metadata": {},
   "outputs": [],
   "source": [
    "The hyperparameters can be tuned to get different accuracy and epsilon values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
